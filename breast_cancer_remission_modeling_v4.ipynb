{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data mining code for the HAP780 final project after exporting the datasets from All of Us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_analysis = pd.read_csv('./data/df_analysis_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Creation: Age as dummy variables in decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins\n",
    "bins = [0, 18, 28, 38, 48, 58, 68, 78, 88, 98, float('inf')]\n",
    "labels = ['<18', '18-27', '28-37', '38-47', '48-57', '58-67', '68-77', '78-87', '88-97', '98+']\n",
    "\n",
    "# Cut the age_at_first_diagnosis into bins\n",
    "df_analysis['age_group'] = pd.cut(df_analysis['age_at_first_diagnosis'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Convert the binned data into dummy variables\n",
    "age_dummies = pd.get_dummies(df_analysis['age_group'])\n",
    "\n",
    "# Concatenate the dummy variables with the original dataframe if needed\n",
    "df_analysis = pd.concat([df_analysis, age_dummies], axis=1)\n",
    "\n",
    "# Drop the 'age_at_first_diagnosis' and 'age_group' columns from the dataframe\n",
    "df_analysis = df_analysis.drop(['age_at_first_diagnosis', 'age_group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_Another single population</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black or African American</th>\n",
       "      <th>race_I prefer not to answer</th>\n",
       "      <th>race_More than one population</th>\n",
       "      <th>race_None Indicated</th>\n",
       "      <th>race_None of these</th>\n",
       "      <th>race_PMI: Skip</th>\n",
       "      <th>race_White</th>\n",
       "      <th>ethnicity_Hispanic or Latino</th>\n",
       "      <th>...</th>\n",
       "      <th>&lt;18</th>\n",
       "      <th>18-27</th>\n",
       "      <th>28-37</th>\n",
       "      <th>38-47</th>\n",
       "      <th>48-57</th>\n",
       "      <th>58-67</th>\n",
       "      <th>68-77</th>\n",
       "      <th>78-87</th>\n",
       "      <th>88-97</th>\n",
       "      <th>98+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 632 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   race_Another single population  race_Asian  race_Black or African American  \\\n",
       "0                               0           0                               0   \n",
       "1                               0           0                               0   \n",
       "2                               0           0                               0   \n",
       "3                               0           0                               0   \n",
       "4                               0           0                               0   \n",
       "\n",
       "   race_I prefer not to answer  race_More than one population  \\\n",
       "0                            0                              0   \n",
       "1                            0                              0   \n",
       "2                            0                              0   \n",
       "3                            0                              0   \n",
       "4                            0                              0   \n",
       "\n",
       "   race_None Indicated  race_None of these  race_PMI: Skip  race_White  \\\n",
       "0                    0                   0               1           0   \n",
       "1                    0                   0               1           0   \n",
       "2                    0                   0               1           0   \n",
       "3                    0                   0               1           0   \n",
       "4                    0                   0               1           0   \n",
       "\n",
       "   ethnicity_Hispanic or Latino  ...  <18  18-27  28-37  38-47  48-57  58-67  \\\n",
       "0                             0  ...    0      0      0      0      0      1   \n",
       "1                             0  ...    0      0      0      0      0      1   \n",
       "2                             0  ...    0      0      0      0      1      0   \n",
       "3                             0  ...    0      0      0      0      1      0   \n",
       "4                             0  ...    0      0      0      0      1      0   \n",
       "\n",
       "   68-77  78-87  88-97  98+  \n",
       "0      0      0      0    0  \n",
       "1      0      0      0    0  \n",
       "2      0      0      0    0  \n",
       "3      0      0      0    0  \n",
       "4      0      0      0    0  \n",
       "\n",
       "[5 rows x 632 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "McFadden's R-squared: 0.0812504501100052\n",
      "\n",
      "Minimum lambda (selected alpha): 0.01685\n",
      "Intercept: 0.20384628503540322\n",
      "\n",
      "Selected features with coefficients:\n",
      "dx_34713006: 0.03732661328729958\n",
      "dx_36923009: 0.02835567241261096\n",
      "dx_40930008: 0.02955844469140588\n",
      "dx_41256004: 0.00170519315765604\n",
      "dx_59621000: 0.0010321843388197852\n",
      "dx_64859006: 0.0032077997316158928\n",
      "dx_76069003: 0.13524425896264614\n",
      "dx_239873007: 0.004939783055518465\n",
      "dx_266435005: 0.06589316687660099\n",
      "dx_400096001: 0.04895022609304274\n"
     ]
    }
   ],
   "source": [
    "# Do feature selection with LASSO limit the number of predictors to 10\n",
    "\n",
    "# Import library\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Set variables\n",
    "X = df_analysis.drop(columns=['No_remission'])\n",
    "y = df_analysis['No_remission']\n",
    "\n",
    "# Initializing Lasso\n",
    "alpha = 0.01685 # manipulate to have 10 predictors\n",
    "lasso = Lasso(alpha = alpha)\n",
    "\n",
    "# Fitting the model\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Calculate McFadden's R-squared\n",
    "# Get predicted probabilities\n",
    "y_pred = lasso.predict(X)\n",
    "\n",
    "# Calculate log-likelihood of the model\n",
    "log_likelihood_model = -log_loss(y, y_pred)\n",
    "\n",
    "# Calculate log-likelihood of the null model\n",
    "null_model_probs = np.full_like(y_pred, y.mean())\n",
    "log_likelihood_null_model = -log_loss(y, null_model_probs)\n",
    "\n",
    "# Calculate McFadden's R-squared\n",
    "mcfadden_r2 = 1 - (log_likelihood_model / log_likelihood_null_model)\n",
    "\n",
    "print(\"\\nMcFadden's R-squared:\", mcfadden_r2)\n",
    "\n",
    "# Print the minimum lambda value selected by LassoCV\n",
    "print(\"\\nMinimum lambda (selected alpha):\", alpha)\n",
    "\n",
    "# Print the intercept\n",
    "print(\"Intercept:\", lasso.intercept_)\n",
    "\n",
    "# Get selected features with abs value of coefficients greater than 0.05\n",
    "selected_features = [(feature, coef) for feature, coef in zip(X.columns, lasso.coef_) if abs(coef) > 0.001]\n",
    "\n",
    "# Print selected feature names and coefficients\n",
    "print(\"\\nSelected features with coefficients:\")\n",
    "for feature, coef in selected_features:\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "\n",
    "# Show results to Dr. Alemi to check if interaction is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes on selected diseases\n",
    "# Vitamin D deficiency (disorder) - dx_34713006: 0.03732661328729958\n",
    "# Major depression, single episode (disorder) - dx_36923009: 0.02835567241261096\n",
    "# Hypothyroidism (disorder) - dx_40930008: 0.02955844469140588\n",
    "# Presbyopia (disorder) - dx_41256004: 0.00170519315765604\n",
    "# Essential hypertension (disorder) - dx_59621000: 0.0010321843388197852\n",
    "# Osteoporosis (disorder) - dx_64859006: 0.0032077997316158928\n",
    "# Disorder of bone (disorder) - dx_76069003: 0.13524425896264614\n",
    "# Osteoarthritis of knee (disorder) - dx_239873007: 0.004939783055518465\n",
    "# Gastroesophageal reflux disease without esophagitis (disorder) - dx_266435005: 0.06589316687660099\n",
    "# Melanocytic nevus (disorder) - dx_400096001: 0.04895022609304274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "McFadden's R-squared: 0.12602614800716316\n",
      "\n",
      "Minimum lambda (selected alpha): 0.00785\n",
      "Intercept: 0.1605714364423537\n",
      "\n",
      "Selected features with coefficients:\n",
      "race_White: -0.017605256791602245\n",
      "dx_11314008: 0.021926136760082943\n",
      "dx_18070006: 0.05414097964901779\n",
      "dx_34713006: 0.047872005975313665\n",
      "dx_36923009: 0.05668637871086478\n",
      "dx_40930008: 0.06140726882908559\n",
      "dx_41256004: 0.028011954366830707\n",
      "dx_46152009: 0.026007346007453754\n",
      "dx_59621000: 0.00983361068412485\n",
      "dx_64859006: 0.020804416115625935\n",
      "dx_65846009: 0.020899711271691268\n",
      "dx_70153002: 0.021459827297058604\n",
      "dx_76069003: 0.1369148676597445\n",
      "dx_193462001: -0.010548064418124942\n",
      "dx_193570009: 0.06890836283817868\n",
      "dx_238810007: -0.0020719296644726506\n",
      "dx_239873007: 0.028903546231882325\n",
      "dx_266435005: 0.07507036440009997\n",
      "dx_271737000: 0.004711805052241484\n",
      "dx_400096001: 0.11086809183568944\n"
     ]
    }
   ],
   "source": [
    "# Do feature selection with LASSO limit the number of predictors to 20\n",
    "\n",
    "# Import library\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Set variables\n",
    "X = df_analysis.drop(columns=['No_remission'])\n",
    "y = df_analysis['No_remission']\n",
    "\n",
    "# Initializing Lasso\n",
    "alpha = 0.00785 # manipulate to have 20 predictors\n",
    "lasso = Lasso(alpha = alpha)\n",
    "\n",
    "# Fitting the model\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Calculate McFadden's R-squared\n",
    "# Get predicted probabilities\n",
    "y_pred = lasso.predict(X)\n",
    "\n",
    "# Calculate log-likelihood of the model\n",
    "log_likelihood_model = -log_loss(y, y_pred)\n",
    "\n",
    "# Calculate log-likelihood of the null model\n",
    "null_model_probs = np.full_like(y_pred, y.mean())\n",
    "log_likelihood_null_model = -log_loss(y, null_model_probs)\n",
    "\n",
    "# Calculate McFadden's R-squared\n",
    "mcfadden_r2 = 1 - (log_likelihood_model / log_likelihood_null_model)\n",
    "\n",
    "print(\"\\nMcFadden's R-squared:\", mcfadden_r2) # Needs to be cross validated\n",
    "\n",
    "# Print the minimum lambda value selected by LassoCV\n",
    "print(\"\\nMinimum lambda (selected alpha):\", alpha)\n",
    "\n",
    "# Print the intercept\n",
    "print(\"Intercept:\", lasso.intercept_)\n",
    "\n",
    "# Get selected features with abs value of coefficients greater than 0.05\n",
    "selected_features = [(feature, coef) for feature, coef in zip(X.columns, lasso.coef_) if abs(coef) > 0.001]\n",
    "\n",
    "# Print selected feature names and coefficients\n",
    "print(\"\\nSelected features with coefficients:\")\n",
    "for feature, coef in selected_features:\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "# Show results to Dr. Alemi to check if interaction is needed\n",
    "\n",
    "# Do interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes on selected diseases\n",
    "# Polyp of corpus uteri (disorder) - dx_11314008: 0.021926136760082943\n",
    "# Impacted cerumen (disorder) - dx_18070006: 0.05414097964901779\n",
    "# Vitamin D deficiency (disorder) - dx_34713006: 0.047872005975313665\n",
    "# Major depression, single episode (disorder) - dx_36923009: 0.05668637871086478\n",
    "# Hypothyroidism (disorder) - dx_40930008: 0.06140726882908559\n",
    "# Presbyopia (disorder) - dx_41256004: 0.028011954366830707\n",
    "# Tear film insufficiency (disorder) - dx_46152009: 0.026007346007453754\n",
    "# Essential hypertension (disorder) - dx_59621000: 0.00983361068412485\n",
    "# Osteoporosis (disorder) - dx_64859006: 0.020804416115625935\n",
    "# Primary ovarian failure (disorder) - dx_65846009: 0.020899711271691268\n",
    "# Hemorrhoids (disorder) - dx_70153002: 0.021459827297058604\n",
    "# Disorder of bone (disorder) - dx_76069003: 0.1369148676597445\n",
    "# Insomnia (disorder) - dx_193462001: -0.010548064418124942\n",
    "# Cataract (disorder) - dx_193570009: 0.06890836283817868\n",
    "# Flushing (disorder) - dx_238810007: -0.0020719296644726506\n",
    "# Osteoarthritis of knee (disorder) - dx_239873007: 0.028903546231882325\n",
    "# Gastroesophageal reflux disease without esophagitis (disorder) - dx_266435005: 0.07507036440009997\n",
    "# Anemia (disorder) - dx_271737000: 0.004711805052241484\n",
    "# Melanocytic nevus (disorder) - dx_400096001: 0.11086809183568944"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_set, test_set = train_test_split(df_analysis, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train and test sets to a file for reference\n",
    "train_set.to_csv(\"./data/train.csv\", index=False)\n",
    "test_set.to_csv(\"./data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             precision_score, \n",
    "                             recall_score, \n",
    "                             f1_score, \n",
    "                             matthews_corrcoef, \n",
    "                             roc_auc_score, \n",
    "                             average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features and target\n",
    "X_train = train_set.drop(columns=['No_remission'])\n",
    "y_train = train_set['No_remission']\n",
    "\n",
    "\n",
    "# Splitting the test data into features and target\n",
    "X_test = test_set.drop(columns=['No_remission'])\n",
    "y_test = test_set['No_remission']\n",
    "\n",
    "# Overwrite selected features lasso\n",
    "selected_features_lasso = [feature for feature, _ in selected_features]\n",
    "\n",
    "X_train_selected_lasso = X_train[selected_features_lasso]\n",
    "X_test_selected_lasso = X_test[selected_features_lasso]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning Of Unbalanced Data Models\n",
    "- Scoring will be based on recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "Confusion Matrix: [[210  14]\n",
      " [ 70  31]]\n",
      "Precision: 0.6888888888888889\n",
      "Recall: 0.3069306930693069\n",
      "F-Measure: 0.4246575342465753\n",
      "MCC: 0.3275330751107881\n",
      "ROC Area: 0.7145067185289957\n",
      "PRC Area: 0.5736680056289689\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Define training sets as unbalanced with feature selection (X_train_selected_lasso, y_train)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso, y_test)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for Logistic Regression\n",
    "grid_search_lr = GridSearchCV(estimator=LogisticRegression(solver='liblinear'), \n",
    "                              param_grid=param_grid_lr, \n",
    "                              scoring=['recall'], \n",
    "                              refit='recall', \n",
    "                              cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_lr.fit(X_train_selected_lasso, y_train)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_lr.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_lr.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "unbal_logreg = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "{'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Confusion Matrix: [[198  26]\n",
      " [ 65  36]]\n",
      "Precision: 0.5806451612903226\n",
      "Recall: 0.3564356435643564\n",
      "F-Measure: 0.44171779141104295\n",
      "MCC: 0.2831266746830076\n",
      "ROC Area: 0.6915001768033947\n",
      "PRC Area: 0.515873955145144\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Define training sets as unbalanced with feature selection (X_train_selected_lasso, y_train)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso, y_test)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), \n",
    "                              param_grid=param_grid_rf, \n",
    "                              scoring=['recall'], \n",
    "                              refit='recall', \n",
    "                              cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_rf.fit(X_train_selected_lasso, y_train)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_rf.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "unbal_rf = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on training set:\n",
      "{'var_smoothing': 0.008111308307896872}\n",
      "Confusion Matrix: [[187  37]\n",
      " [ 53  48]]\n",
      "Precision: 0.5647058823529412\n",
      "Recall: 0.4752475247524752\n",
      "F-Measure: 0.5161290322580645\n",
      "MCC: 0.3265336144278757\n",
      "ROC Area: 0.7092910183875529\n",
      "PRC Area: 0.5495002642841829\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "# Define training sets as unbalanced with feature selection (X_train_selected_lasso, y_train)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso, y_test)\n",
    "\n",
    "# Define the parameter grid for GaussianNB\n",
    "param_grid_gnb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for GaussianNB\n",
    "grid_search_gnb = GridSearchCV(estimator=GaussianNB(), \n",
    "                               param_grid=param_grid_gnb, \n",
    "                               scoring=['recall'], \n",
    "                               refit='recall', \n",
    "                               cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_gnb.fit(X_train_selected_lasso, y_train)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_gnb.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_gnb.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "unbal_nb = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "# Define training sets as unbalanced with feature selection (X_train_selected_lasso, y_train)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso, y_test)\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'colsample_bytree': [0.3, 0.7, 1]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for XGBoost\n",
    "grid_search_xgb = GridSearchCV(estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "                               param_grid=param_grid_xgb, \n",
    "                               scoring=['recall'], \n",
    "                               refit='recall', \n",
    "                               cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_xgb.fit(X_train_selected_lasso, y_train)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_xgb.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "unbal_xgb = best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curves using specificity and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': unbal_logreg,\n",
    "    'Random Forest': unbal_rf,\n",
    "    'Naive Bayes': unbal_nb,\n",
    "    'XGBoost': unbal_xgb,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curve and ROC AUC for each model\n",
    "for name, model in models.items():\n",
    "    probas_ = model.predict_proba(X_test_selected_lasso)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate precision-recall curve and AUC for each model\n",
    "for name, model in models.items():\n",
    "    probas_ = model.predict_proba(X_test_selected_lasso)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, probas_[:, 1])\n",
    "    auprc = auc(recall, precision)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{name} (area = {auprc:.2f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall (Sensitivity)')\n",
    "plt.ylabel('Precision (Positive Predictive Value)')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower right\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race Mix and Ethnicity Mix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter test set for particular race or ethnicity\n",
    "\n",
    "# Non-Hispanic White\n",
    "test_set_NHWhite = test_set[(test_set['race_White'] == 1) & (test_set['ethnicity_Not Hispanic or Latino'] == 1)]\n",
    "X_test_NHWhite = test_set_NHWhite.drop(columns=['No_remission'])\n",
    "X_test_NHWhite = X_test_NHWhite[selected_features_lasso]\n",
    "y_test_NHWhite = test_set_NHWhite['No_remission']\n",
    "\n",
    "# Non-Hispanic Black\n",
    "test_set_NHBlack = test_set[(test_set['race_Black or African American'] == 1) & (test_set['ethnicity_Not Hispanic or Latino'] == 1)]\n",
    "X_test_NHBlack = test_set_NHBlack.drop(columns=['No_remission'])\n",
    "X_test_NHBlack = X_test_NHBlack[selected_features_lasso]\n",
    "y_test_NHBlack = test_set_NHBlack['No_remission']\n",
    "\n",
    "# Hispanic\n",
    "test_set_Hispanic = test_set[test_set['ethnicity_Hispanic or Latino'] == 1]\n",
    "X_test_Hispanic = test_set_Hispanic.drop(columns=['No_remission'])\n",
    "X_test_Hispanic = X_test_Hispanic[selected_features_lasso]\n",
    "y_test_Hispanic = test_set_Hispanic['No_remission']\n",
    "\n",
    "# Other\n",
    "test_set_Other = test_set[~(((test_set['race_White'] == 1) & (test_set['ethnicity_Not Hispanic or Latino'] == 1)) | \n",
    "                            ((test_set['race_Black or African American'] == 1) & (test_set['ethnicity_Not Hispanic or Latino'] == 1)) | \n",
    "                            (test_set['ethnicity_Hispanic or Latino'] == 1))]\n",
    "X_test_Other = test_set_Other.drop(columns=['No_remission'])\n",
    "X_test_Other = X_test_Other[selected_features_lasso]\n",
    "y_test_Other = test_set_Other['No_remission']\n",
    "\n",
    "# Number of selected rows\n",
    "print(f\"\"\"Non-Hispanic White: {test_set_NHWhite.shape[0]} | \n",
    "      Non-Hispanic Black: {test_set_NHBlack.shape[0]} | \n",
    "      Hispanic: {test_set_Hispanic.shape[0]} | \n",
    "      Other: {test_set_Other.shape[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models on Non-Hispanic White test set\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression (Unbalanced)': unbal_logreg,\n",
    "    'Random Forest (Unbalanced)': unbal_rf,\n",
    "    'Naive Bayes (Unbalanced)': unbal_nb,\n",
    "    'XGBoost (Unbalanced)': unbal_xgb\n",
    "}\n",
    "\n",
    "# Metrics collection\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_NHWhite)\n",
    "    \n",
    "    # Metrics\n",
    "    confusion = confusion_matrix(y_test_NHWhite, y_pred)\n",
    "    precision = precision_score(y_test_NHWhite, y_pred)\n",
    "    recall = recall_score(y_test_NHWhite, y_pred)\n",
    "    fmeasure = f1_score(y_test_NHWhite, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test_NHWhite, y_pred)\n",
    "    roc_area = roc_auc_score(y_test_NHWhite, model.predict_proba(X_test_NHWhite)[:, 1])\n",
    "    prc_area = average_precision_score(y_test_NHWhite, model.predict_proba(X_test_NHWhite)[:, 1])\n",
    "    \n",
    "    results[name] = {\n",
    "        'Confusion Matrix': confusion,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F-Measure': fmeasure,\n",
    "        'MCC': mcc,\n",
    "        'ROC Area': roc_area,\n",
    "        'PRC Area': prc_area\n",
    "    }\n",
    "    \n",
    "# Display results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models on Non-Hispanic Black test set\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression (Unbalanced)': unbal_logreg,\n",
    "    'Random Forest (Unbalanced)': unbal_rf,\n",
    "    'Naive Bayes (Unbalanced)': unbal_nb,\n",
    "    'XGBoost (Unbalanced)': unbal_xgb\n",
    "}\n",
    "\n",
    "# Metrics collection\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_NHBlack)\n",
    "    \n",
    "    # Metrics\n",
    "    confusion = confusion_matrix(y_test_NHBlack, y_pred)\n",
    "    precision = precision_score(y_test_NHBlack, y_pred)\n",
    "    recall = recall_score(y_test_NHBlack, y_pred)\n",
    "    fmeasure = f1_score(y_test_NHBlack, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test_NHBlack, y_pred)\n",
    "    roc_area = roc_auc_score(y_test_NHBlack, model.predict_proba(X_test_NHBlack)[:, 1])\n",
    "    prc_area = average_precision_score(y_test_NHBlack, model.predict_proba(X_test_NHBlack)[:, 1])\n",
    "    \n",
    "    results[name] = {\n",
    "        'Confusion Matrix': confusion,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F-Measure': fmeasure,\n",
    "        'MCC': mcc,\n",
    "        'ROC Area': roc_area,\n",
    "        'PRC Area': prc_area\n",
    "    }\n",
    "    \n",
    "# Display results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models on Hispanic test set\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression (Unbalanced)': unbal_logreg,\n",
    "    'Random Forest (Unbalanced)': unbal_rf,\n",
    "    'Naive Bayes (Unbalanced)': unbal_nb,\n",
    "    'XGBoost (Unbalanced)': unbal_xgb\n",
    "}\n",
    "\n",
    "# Metrics collection\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_Hispanic)\n",
    "    \n",
    "    # Metrics\n",
    "    confusion = confusion_matrix(y_test_Hispanic, y_pred)\n",
    "    precision = precision_score(y_test_Hispanic, y_pred)\n",
    "    recall = recall_score(y_test_Hispanic, y_pred)\n",
    "    fmeasure = f1_score(y_test_Hispanic, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test_Hispanic, y_pred)\n",
    "    roc_area = roc_auc_score(y_test_Hispanic, model.predict_proba(X_test_Hispanic)[:, 1])\n",
    "    prc_area = average_precision_score(y_test_Hispanic, model.predict_proba(X_test_Hispanic)[:, 1])\n",
    "    \n",
    "    results[name] = {\n",
    "        'Confusion Matrix': confusion,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F-Measure': fmeasure,\n",
    "        'MCC': mcc,\n",
    "        'ROC Area': roc_area,\n",
    "        'PRC Area': prc_area\n",
    "    }\n",
    "    \n",
    "# Display results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models on \"Other\" test set\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression (Unbalanced)': unbal_logreg,\n",
    "    'Random Forest (Unbalanced)': unbal_rf,\n",
    "    'Naive Bayes (Unbalanced)': unbal_nb,\n",
    "    'XGBoost (Unbalanced)': unbal_xgb\n",
    "}\n",
    "\n",
    "# Metrics collection\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_Other)\n",
    "    \n",
    "    # Metrics\n",
    "    confusion = confusion_matrix(y_test_Other, y_pred)\n",
    "    precision = precision_score(y_test_Other, y_pred)\n",
    "    recall = recall_score(y_test_Other, y_pred)\n",
    "    fmeasure = f1_score(y_test_Other, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test_Other, y_pred)\n",
    "    roc_area = roc_auc_score(y_test_Other, model.predict_proba(X_test_Other)[:, 1])\n",
    "    prc_area = average_precision_score(y_test_Other, model.predict_proba(X_test_Other)[:, 1])\n",
    "    \n",
    "    results[name] = {\n",
    "        'Confusion Matrix': confusion,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F-Measure': fmeasure,\n",
    "        'MCC': mcc,\n",
    "        'ROC Area': roc_area,\n",
    "        'PRC Area': prc_area\n",
    "    }\n",
    "    \n",
    "# Display results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': unbal_logreg,\n",
    "    'Random Forest': unbal_rf,\n",
    "    'Naive Bayes': unbal_nb,\n",
    "    'XGBoost': unbal_xgb,\n",
    "}\n",
    "\n",
    "# Define race-mix test sets\n",
    "rm = {\n",
    "    'Non-Hispanic White' : [X_test_NHWhite, y_test_NHWhite],\n",
    "    'Non-Hispanic Black' : [X_test_NHBlack, y_test_NHBlack],\n",
    "    'Hispanic' : [X_test_Hispanic, y_test_Hispanic],\n",
    "    'Other Ethnicity-Race' : [X_test_Other, y_test_Other]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curve and ROC AUC for each model\n",
    "for name, model in models.items():\n",
    "    for rm_name, rm_test_set in rm.items():\n",
    "        probas_ = model.predict_proba(rm_test_set[0])\n",
    "        fpr, tpr, thresholds = roc_curve(rm_test_set[1], probas_[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{rm_name}: {name}  (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate precision-recall curve and AUC for each model\n",
    "for name, model in models.items():\n",
    "    for rm_name, rm_test_set in rm.items():\n",
    "        probas_ = model.predict_proba(rm_test_set[0])\n",
    "        precision, recall, thresholds = precision_recall_curve(rm_test_set[1], probas_[:, 1])\n",
    "        auprc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, lw=2, label=f'{rm_name}: {name} (area = {auprc:.2f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall (Sensitivity)')\n",
    "plt.ylabel('Precision (Positive Predictive Value)')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower right\") \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
