{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data mining code for the HAP780 final project after exporting the datasets from All of Us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df_analysis = pd.read_csv('./data/df_analysis_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency # for chi-square tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Creation: Age as dummy variables in decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins\n",
    "bins = [0, 18, 28, 38, 48, 58, 68, 78, 88, 98, float('inf')]\n",
    "labels = ['<18', '18-27', '28-37', '38-47', '48-57', '58-67', '68-77', '78-87', '88-97', '98+']\n",
    "\n",
    "# Cut the age_at_first_diagnosis into bins\n",
    "df_analysis['age_group'] = pd.cut(df_analysis['age_at_first_diagnosis'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Convert the binned data into dummy variables\n",
    "age_dummies = pd.get_dummies(df_analysis['age_group'])\n",
    "\n",
    "# Concatenate the dummy variables with the original dataframe if needed\n",
    "df_analysis = pd.concat([df_analysis, age_dummies], axis=1)\n",
    "\n",
    "# Drop the 'age_at_first_diagnosis' and 'age_group' columns from the dataframe\n",
    "df_analysis = df_analysis.drop(['age_at_first_diagnosis', 'age_group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Creation: Create Interaction Variables (2-way only due to memory constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target variable\n",
    "X = df_analysis.drop(columns=['No_remission'])\n",
    "\n",
    "# Create polynomial features (interaction terms only)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Create a dataframe for the interaction terms\n",
    "df_interactions = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(X.columns))\n",
    "\n",
    "# Identify interaction terms only (exclude original feature columns)\n",
    "interaction_columns = df_interactions.columns[~df_interactions.columns.isin(X.columns)]\n",
    "\n",
    "# Join the original df_analysis with the interaction terms dataframe\n",
    "df_analysis_extended = pd.concat([df_analysis, df_interactions[interaction_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning: Remove columns that have all zeroes or all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns that are all ones or all zeroes for dropping\n",
    "columns_to_drop = df_analysis_extended.columns[(\n",
    "    df_analysis_extended.sum(axis=0) == len(df_analysis_extended)) | (df_analysis_extended.sum(axis=0) == 0)]\n",
    "\n",
    "# Drop these columns\n",
    "df_analysis_extended = df_analysis_extended.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_extended.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into Training (80%) and Test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "train_set, test_set = train_test_split(df_analysis_extended, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline models without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             precision_score, \n",
    "                             recall_score, \n",
    "                             f1_score, \n",
    "                             matthews_corrcoef, \n",
    "                             roc_auc_score, \n",
    "                             average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into features and target\n",
    "X_train = train_set.drop(columns=['No_remission'])\n",
    "y_train = train_set['No_remission']\n",
    "\n",
    "\n",
    "# Splitting the test data into features and target\n",
    "X_test = test_set.drop(columns=['No_remission'])\n",
    "y_test = test_set['No_remission']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection: LASSO Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing LassoCV (5 folds)\n",
    "lasso_cv = LassoCV(cv=5, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature coefficients\n",
    "coef = pd.Series(lasso_cv.coef_, index=X_train.columns)\n",
    "\n",
    "# Filter out the features which have a coefficient of zero\n",
    "selected_features_lasso = coef[coef != 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected feature and direction\n",
    "print(f\"Number of features selected: {len(selected_features_lasso)}\\n\")\n",
    "\n",
    "for feature, coef in zip(selected_features_lasso, lasso_cv.coef_):\n",
    "    effect = 'increase' if coef > 0 else 'decrease' if coef < 0 else 'no effect'\n",
    "    print(f\"feature: '{feature}' | effect: {effect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter train and test sets for selected features\n",
    "X_train_selected_lasso = X_train[selected_features_lasso]\n",
    "X_test_selected_lasso = X_test[selected_features_lasso]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Balancing Using Synthetic Minority Over-sampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Resampled sample size\n",
    "print(f\"Original training sample size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training sample size: {X_resampled.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Testing Using Balanced Data and Selected Features (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter train and test sets for selected features\n",
    "X_train_selected_lasso_2 = X_resampled[selected_features_lasso]\n",
    "X_test_selected_lasso_2 = X_test[selected_features_lasso]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=5000),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Metrics collection\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_selected_lasso_2, y_resampled)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_selected_lasso_2)\n",
    "    \n",
    "    # Metrics\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    fmeasure = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    roc_area = roc_auc_score(y_test, model.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "    prc_area = average_precision_score(y_test, model.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "    \n",
    "    results[name] = {\n",
    "        'Confusion Matrix': confusion,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F-Measure': fmeasure,\n",
    "        'MCC': mcc,\n",
    "        'ROC Area': roc_area,\n",
    "        'PRC Area': prc_area\n",
    "    }\n",
    "    \n",
    "# Display results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning Of Best Data and Feature Combination On Recall\n",
    "- Scoring will be based on recall\n",
    "- Chosen training set with highest recall: Balanced with Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Define training sets as balanced with feature selection (X_train_selected_lasso_2, y_resampled)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso_2)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for Logistic Regression\n",
    "grid_search_lr = GridSearchCV(estimator=LogisticRegression(solver='liblinear'), \n",
    "                              param_grid=param_grid_lr, \n",
    "                              scoring=['recall'], \n",
    "                              refit='recall', \n",
    "                              cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_lr.fit(X_train_selected_lasso_2, y_resampled)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_lr.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_lr.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso_2)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "best_logreg = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Define training sets as balanced with feature selection (X_train_selected_lasso_2, y_resampled)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso_2)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(), \n",
    "                              param_grid=param_grid_rf, \n",
    "                              scoring=['recall'], \n",
    "                              refit='recall', \n",
    "                              cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_rf.fit(X_train_selected_lasso_2, y_resampled)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_rf.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso_2)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "best_rf = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "# Define training sets as balanced with feature selection (X_train_selected_lasso_2, y_resampled)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso_2)\n",
    "\n",
    "# Define the parameter grid for GaussianNB\n",
    "param_grid_gnb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for GaussianNB\n",
    "grid_search_gnb = GridSearchCV(estimator=GaussianNB(), \n",
    "                               param_grid=param_grid_gnb, \n",
    "                               scoring=['recall'], \n",
    "                               refit='recall', \n",
    "                               cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_gnb.fit(X_train_selected_lasso_2, y_resampled)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_gnb.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_gnb.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso_2)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "best_nb = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "# Define training sets as balanced with feature selection (X_train_selected_lasso_2, y_resampled)\n",
    "# Define test set as balanced with feature selection (X_test_selected_lasso_2)\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'colsample_bytree': [0.3, 0.7, 1]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for XGBoost\n",
    "grid_search_xgb = GridSearchCV(estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "                               param_grid=param_grid_xgb, \n",
    "                               scoring=['recall'], \n",
    "                               refit='recall', \n",
    "                               cv=5)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_xgb.fit(X_train_selected_lasso_2, y_resampled)\n",
    "\n",
    "# After fitting, we can check the best performance in the training set\n",
    "print(\"Best parameters set found on training set:\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "\n",
    "# Predict\n",
    "best_estimator = grid_search_xgb.best_estimator_\n",
    "y_pred = best_estimator.predict(X_test_selected_lasso_2)\n",
    "    \n",
    "# Metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "fmeasure = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "roc_area = roc_auc_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "prc_area = average_precision_score(y_test, best_estimator.predict_proba(X_test_selected_lasso_2)[:, 1])\n",
    "    \n",
    "results = {\n",
    "    'Confusion Matrix': confusion,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F-Measure': fmeasure,\n",
    "    'MCC': mcc,\n",
    "    'ROC Area': roc_area,\n",
    "    'PRC Area': prc_area\n",
    "}\n",
    "    \n",
    "# Display results\n",
    "for metric_name, metric_value in results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "# Save best estimator for plotting\n",
    "best_xgb = best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curves for the hyperparameter tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual models and test data\n",
    "models = {\n",
    "    'Logistic Regression': best_logreg,\n",
    "    'Random Forest': best_rf,\n",
    "    'Naive Bayes': best_nb,\n",
    "    'XGBoost': best_xgb\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curve and ROC AUC for each model\n",
    "for name, model in models.items():\n",
    "    probas_ = model.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
